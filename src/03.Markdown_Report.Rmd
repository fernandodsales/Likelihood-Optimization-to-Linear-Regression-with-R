---
title: "Likelihood Optimization to Linear Regression with R"
author: "Fernando Delgado"
output:
  html_document:
    theme: cerulean
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, echo = FALSE, include = FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/fdelgado/OneDrive - IESEG/Documents/01. IESEG/17. Optimization/Individual Project")

for (i in c('dplyr','tidytext','tidyverse','data.table', 'aplore3')){
  if (!require(i, character.only=TRUE)) install.packages(i, repos = "http://cran.us.r-project.org")
  require(i, character.only=TRUE)
}
for (i in c('ggplot2','scales','maps','maptools','ggmap')){
  if (!require(i, character.only=TRUE)) install.packages(i, repos = "http://cran.us.r-project.org")
  require(i, character.only=TRUE)
}

library(knitr)

```
## Introduction

Linear regression is a statistic model used for predicting a numerical quantity. The parameters of a linear regression model can be estimated using least squares or by maximum likelihood optimization. Maximum likelihood estimation is a probabilistic framework for automatically finding the probability distribution for the observed data. 

Through this report, we perform a Likelihood Optimization to linear regression with R. 

## Loading the Data 

First, we simulate a dataframe to work with:

```{r eval=FALSE}

# Create a first column to test 
a <- rnorm(1000,0)
df <- a

# Create 5 columns with random data
columns <- cbind(1:5)

# For loop data into columns
for (i in columns){
  name <- paste("var",i, sep="")
  tmp <- rnorm(1000,0)
  df <- data.frame(df, tmp)
  names(df)[names(df) == 'tmp'] <- name
}

```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
fake_arrests <- fread("./data/fake_arrests.csv")
fake_arrests <- subset(fake_arrests, select = -V1)
```

We create a dataframe with `r nrow(fake_arrests)` observations to  `r ncol(fake_arrests) - 1`  predictor variables using R’s rnorm() assignin a mean of 0 to obtain normalized random data.

Then, to give a little bit of sense to our data, we use USArrests built-in data set and fakeR’s simulate_data() to obtain a random target variable of Arrests by Assaults by city. We obtain 1000 random observations that we append to our previous dataset. 
The final result is a data frame with 1000 observations for 5 random predictor variables to 1 target variable (number of arrests by assault by city).   

```{r, echo=FALSE, warning=FALSE, message=FALSE}
str(fake_arrests)
```

## Linear Regression 

To optimize a function, it is important to consider what function we should optimize. To work with a linear regression function in this case, we know that it follows a normal distribution with a mean of 0 and an unknown standard deviation. 

$\sum_{i=1}^{i=n} R_{i} \~ N(0,s)$

Where R equals:

$R_{i} = y{1} - \hat{y_{i}}$

Therefore, the objective is obtaining a function of $y_{i}$ which minimizes the residuals and shows the best coefficients for our function. 

With this in mind, we write an optimization function with the following code:

```{r}

#Define likelihood function to optimise
ll_lm <- function(par, y, x1, x2, x3, x4, x5){
  
  alpha <- par[1]
  beta1 <- par[2]
  beta2 <- par[3]
  beta3 <- par[4]
  beta4 <- par[5]
  beta5 <- par[6]
  sigma <- par[7]
  
  R = y - alpha - beta1 * x1 - beta2 * x2 - beta3 * x3 - beta4 * x4 - beta5 * x5
  
  -sum(dnorm(R, mean = 0, sigma, log = TRUE))
}

```

Alpha refers to our target variable, and the 5 betas represent each predictor variable. 

In order to use the optim() function, it must have par arguments. Par arguments need a vector with guesses for all unknown parameters. In our code above, par arguments include initial values in all 7 of the unknown parameters. 

It is important to mention that by using dnorm() we obtain logarithmic values. This helps to sum the single likelihood values instead of the product. 

The linear model we are fitting looks like this:

$E(Y|X) = \alpha + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{3} +  \beta_{4}x_{4} + \beta_{5}x_{5}$

Therefore, the residuals are calculated like this:

$R = y - \alpha - \beta_{1}x_{1} - \beta_{2}x_{2} - \beta_{3}x_{3} - \beta_{4}x_{4} - \beta_{5}x_{5}$

Moreover, since residuals are following a normal distribution with a mean of 0, what’s left Is to find the standard deviation that best fits our data. Hence, we minimize the sum of errors with optim() command by using a minus sign before the sum.

However, before running the optimization, we also estimate our coefficients by simply calculating the mean of each variable:

```{r}
#Estimate Betas
est_alpha <- mean(fake_arrests$Assault)
est_beta1 <- mean(fake_arrests$var1)
est_beta2 <- mean(fake_arrests$var2)
est_beta3 <- mean(fake_arrests$var3)
est_beta4 <- mean(fake_arrests$var4)
est_beta5 <- mean(fake_arrests$var5)
est_sigma <- sd(fake_arrests$Assault)
```

To keep it simple, we do it manually, but this could be looped for a larger dataset. 

Moving forward, we optimize our model searching our maximum likelihood estimates for the different coefficients. We introduce our function ll_lm(), the estimated coefficients and the data to be optimized:

```{r}
mle_par <- optim(fn = ll_lm,               
                 par = c(alpha = est_alpha, 
                         beta1 = est_beta1, 
                         beta2 = est_beta2,
                         beta3 = est_beta3,
                         beta4 = est_beta4,
                         beta5 = est_beta5,
                         sigma = est_sigma), 
                 y = fake_arrests$Assault,               
                 x1 = fake_arrests$var1,
                 x2 = fake_arrests$var2,
                 x3 = fake_arrests$var3,
                 x4 = fake_arrests$var4,
                 x5 = fake_arrests$var5)

```

Finally, we obtain our estimated coefficients 

```{r, echo=FALSE}
mle_par$par

```

## Validation 

If we compare the estimate with the result of the lm() command for the same model, we observe some slight differences in the coefficients. However, since they are rather small it is probably due to our initial guesses for the parameters. 


```{r, echo=FALSE}
summary(lm(Assault ~ var1 + var2 + var3 + var4 + var5, data = fake_arrests))

```

## References

Creating fake simulated data was inspired from this following post:

https://rviews.rstudio.com/2020/09/09/fake-data-with-r/

Maximization of likelihood was inspired by this following post:

https://www.joshua-entrop.com/post/optim_linear_reg/
